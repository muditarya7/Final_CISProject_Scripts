# -*- coding: utf-8 -*-
"""spoken_mnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ihxvb697Ue2LiXCqRiVGgan66roGJGVV
"""

# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y6yNV9wpm4R7gAHxTC0MEs5cMGaN6-My
"""

!pip install hub

!pip install fairseq

import torch
import torch.nn as nn
from fairseq.models.wav2vec import wav2vec2, TransformerEncoder
from fairseq.dataclass.utils import convert_namespace_to_omegaconf
from argparse import Namespace
from omegaconf import OmegaConf
from torch.nn import TransformerEncoderLayer
import torch.optim as optim

args = Namespace(
    encoder_embed_dim=768,
    encoder_layers=12,
    encoder_layerdrop=0.05,
    dropout=0.1,
    attention_dropout=0.1,
    activation_dropout=0,
    dropout_input=0.1,
    final_dim=0,
    layer_norm_first=False,
    conv_feature_layers='[(512, 10, 5)]',
    conv_pos=512,
    conv_pos_groups=16,
    encoder_ffn_embed_dim=3072,
    encoder_attention_heads=12,
    activation_fn='gelu',
)


config = wav2vec2.Wav2Vec2Config.from_namespace(args)

class DeCoAR2(nn.Module):
    def __init__(self):
        super(DeCoAR2, self).__init__()
        self.linear = nn.Linear(64 * 64 * 4, 768)
        self.bn = nn.BatchNorm1d(768)  # Batch normalization
        self.dropout = nn.Dropout(0.3)
        self.transformer = TransformerEncoderLayer(d_model=768, nhead=12, dropout=0.1)
        self.output_layer = nn.Linear(768, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.linear(x)
        x = self.bn(x)
        x = self.dropout(x)
        x = x.unsqueeze(1)
        x = self.transformer(x)
        x = x.squeeze(1)
        x = self.output_layer(x)
        return x

# Training enhancements
model=DeCoAR2()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchaudio
import torchaudio.transforms as T
from fairseq.models.wav2vec import TransformerEncoder, wav2vec2

class AudioDataset(Dataset):
    def __init__(self, audio_files, labels, transform=None):
        self.audio_files = audio_files
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.audio_files)

    def __getitem__(self, idx):
        waveform, sample_rate = torchaudio.load(self.audio_files[idx])
        label = self.labels[idx]
        if self.transform:
            waveform = self.transform(waveform)
        return waveform.squeeze(0), label

import hub

# Load the dataset from Hub
hub_dataset = hub.load("hub://activeloop/spoken_mnist")

import numpy as np
from sklearn.model_selection import train_test_split


spectrograms = hub_dataset['spectrograms'].numpy()
labels = hub_dataset['labels'].numpy()


train_specs, test_specs, train_labels, test_labels = train_test_split(
    spectrograms, labels, test_size=0.2, random_state=42
)

import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np


if isinstance(train_specs, np.ndarray):
    train_specs = torch.tensor(train_specs, dtype=torch.float32)
    print("Converted train_specs to tensor with shape:", train_specs.shape)
if isinstance(test_specs, np.ndarray):
    test_specs = torch.tensor(test_specs, dtype=torch.float32)
    print("Converted test_specs to tensor with shape:", test_specs.shape)

if isinstance(train_labels, np.ndarray):
    train_labels = torch.tensor(train_labels.astype(np.int64), dtype=torch.long)
    print("Converted train_labels to tensor with shape:", train_labels.shape)
else:
    train_labels = train_labels.to(dtype=torch.long)
    print("Converted train_labels to tensor with shape:", train_labels.shape)

if isinstance(test_labels, np.ndarray):
    test_labels = torch.tensor(test_labels.astype(np.int64), dtype=torch.long)
    print("Converted test_labels to tensor with shape:", test_labels.shape)
else:
    test_labels = test_labels.to(dtype=torch.long)
    print("Converted test_labels to tensor with shape:", test_labels.shape)


train_dataset = TensorDataset(train_specs, train_labels)
test_dataset = TensorDataset(test_specs, test_labels)

print("Train dataset size:", len(train_dataset))
print("Test dataset size:", len(test_dataset))


train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print("Train DataLoader size (batches):", len(train_loader))
print("Test DataLoader size (batches):", len(test_loader))

print("Training Dataset Size:", len(train_loader.dataset))
print("Validation Dataset Size:", len(test_loader.dataset))


for mel_specs, labels in train_loader:
    print("Sample Labels:", labels[:10])
    break

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

model = DeCoAR2()
if torch.cuda.is_available():
    model.cuda()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)
scheduler = StepLR(optimizer, step_size=1, gamma=0.002)

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    total_train_correct = 0
    total_train_samples = 0

    for mel_specs, labels in train_loader:
        if torch.cuda.is_available():
            mel_specs, labels = mel_specs.cuda(), labels.cuda()

        labels = labels.squeeze()

        optimizer.zero_grad()
        outputs = model(mel_specs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total_train_correct += (predicted == labels).sum().item()
        total_train_samples += labels.size(0)

    scheduler.step()

    average_train_loss = total_train_loss / len(train_loader)
    train_accuracy = 100 * total_train_correct / total_train_samples

    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')